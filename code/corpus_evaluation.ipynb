{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSxjKzDnWiIy",
        "outputId": "3794fc75-7a2e-4654-b120-e30408f0886e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded corpus: 261 sentences, 21 classes\n",
            "=== FINAL BIAS METRICS ===\n",
            "CEAT: 2.00\n",
            "I-WEAT: 2.10\n",
            "I-SEAT: 1.80\n",
            "IIBS: 0.50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "def load_corpus(file_path, encoding='latin-1'):\n",
        "    \"\"\"Load the corpus CSV file\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding='utf-8')\n",
        "    except:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding='latin-1')\n",
        "        except:\n",
        "            df = pd.read_csv(file_path, encoding='cp1252')\n",
        "    return df\n",
        "\n",
        "def define_intersectional_targets():\n",
        "    \"\"\"Define target groups and attributes for intersectional analysis\"\"\"\n",
        "    return {\n",
        "        'Race + Region + Tech-Ethics': {\n",
        "            'targets': ['Black', 'African', 'Asian', 'Latino', 'indigenous', 'minority', 'ethnic'],\n",
        "            'attributes_positive': ['innovative', 'ethical', 'responsible', 'advanced', 'progressive'],\n",
        "            'attributes_negative': ['backward', 'unethical', 'irresponsible', 'primitive', 'limited']\n",
        "        },\n",
        "        'Gender + Race + Public-Health': {\n",
        "            'targets': ['women', 'Black women', 'Latino women', 'Asian women', 'minority women'],\n",
        "            'attributes_positive': ['healthy', 'informed', 'compliant', 'educated', 'aware'],\n",
        "            'attributes_negative': ['unhealthy', 'ignorant', 'non-compliant', 'uneducated', 'unaware']\n",
        "        },\n",
        "        'Class + Age + Career-Wealth': {\n",
        "            'targets': ['poor', 'working-class', 'elderly', 'young', 'low-income'],\n",
        "            'attributes_positive': ['successful', 'wealthy', 'ambitious', 'capable', 'skilled'],\n",
        "            'attributes_negative': ['unsuccessful', 'poor', 'lazy', 'incapable', 'unskilled']\n",
        "        },\n",
        "        'Disability + Region + Education-Access': {\n",
        "            'targets': ['disabled', 'handicapped', 'rural', 'remote', 'disadvantaged'],\n",
        "            'attributes_positive': ['educated', 'capable', 'intelligent', 'skilled', 'competent'],\n",
        "            'attributes_negative': ['uneducated', 'incapable', 'stupid', 'unskilled', 'incompetent']\n",
        "        },\n",
        "        'Appearance + Gender + Ethnicity': {\n",
        "            'targets': ['women', 'ethnic', 'minority', 'Black', 'Asian', 'Latino'],\n",
        "            'attributes_positive': ['professional', 'attractive', 'presentable', 'appropriate', 'suitable'],\n",
        "            'attributes_negative': ['unprofessional', 'unattractive', 'inappropriate', 'unsuitable', 'backward']\n",
        "        },\n",
        "        'Culture/Tradition + Age + Workplace': {\n",
        "            'targets': ['traditional', 'cultural', 'elderly', 'conservative', 'old-fashioned'],\n",
        "            'attributes_positive': ['modern', 'innovative', 'progressive', 'adaptable', 'flexible'],\n",
        "            'attributes_negative': ['outdated', 'rigid', 'inflexible', 'backward', 'conservative']\n",
        "        }\n",
        "    }\n",
        "\n",
        "def calculate_ceat_score(sentences, targets, pos_attrs, neg_attrs):\n",
        "    \"\"\"Calculate CEAT-like score using TF-IDF similarity\"\"\"\n",
        "    target_scores = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_lower = sentence.lower()\n",
        "\n",
        "        # Check for target words\n",
        "        target_count = sum(1 for target in targets if target.lower() in sentence_lower)\n",
        "\n",
        "        # Check for positive/negative attributes\n",
        "        pos_count = sum(1 for attr in pos_attrs if attr.lower() in sentence_lower)\n",
        "        neg_count = sum(1 for attr in neg_attrs if attr.lower() in sentence_lower)\n",
        "\n",
        "        if target_count > 0:\n",
        "            if pos_count > 0:\n",
        "                target_scores.append(1.0)  # Positive association\n",
        "            elif neg_count > 0:\n",
        "                target_scores.append(-1.0)  # Negative association\n",
        "            else:\n",
        "                target_scores.append(0.0)  # Neutral\n",
        "\n",
        "    return np.mean(target_scores) if len(target_scores) > 0 else 0.0\n",
        "\n",
        "def calculate_iweat_score(sentences, targets, pos_attrs, neg_attrs):\n",
        "    \"\"\"Calculate I-WEAT-like score based on word co-occurrence\"\"\"\n",
        "    target_pos_cooccurrence = 0\n",
        "    target_neg_cooccurrence = 0\n",
        "    total_target_sentences = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_lower = sentence.lower()\n",
        "        has_target = any(target.lower() in sentence_lower for target in targets)\n",
        "\n",
        "        if has_target:\n",
        "            total_target_sentences += 1\n",
        "            pos_attrs_in_sentence = sum(1 for attr in pos_attrs if attr.lower() in sentence_lower)\n",
        "            neg_attrs_in_sentence = sum(1 for attr in neg_attrs if attr.lower() in sentence_lower)\n",
        "\n",
        "            target_pos_cooccurrence += pos_attrs_in_sentence\n",
        "            target_neg_cooccurrence += neg_attrs_in_sentence\n",
        "\n",
        "    if total_target_sentences == 0:\n",
        "        return 0.0\n",
        "\n",
        "    pos_ratio = target_pos_cooccurrence / total_target_sentences\n",
        "    neg_ratio = target_neg_cooccurrence / total_target_sentences\n",
        "\n",
        "    return pos_ratio - neg_ratio\n",
        "\n",
        "def calculate_iseat_score(sentences, targets, pos_attrs, neg_attrs):\n",
        "    \"\"\"Calculate I-SEAT-like score using sentence-level analysis\"\"\"\n",
        "    sentence_scores = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_lower = sentence.lower()\n",
        "        target_present = any(target.lower() in sentence_lower for target in targets)\n",
        "\n",
        "        if target_present:\n",
        "            pos_count = sum(1 for attr in pos_attrs if attr.lower() in sentence_lower)\n",
        "            neg_count = sum(1 for attr in neg_attrs if attr.lower() in sentence_lower)\n",
        "\n",
        "            if pos_count > neg_count:\n",
        "                sentence_scores.append(1.0)\n",
        "            elif neg_count > pos_count:\n",
        "                sentence_scores.append(-1.0)\n",
        "            else:\n",
        "                sentence_scores.append(0.0)\n",
        "\n",
        "    return np.mean(sentence_scores) if len(sentence_scores) > 0 else 0.0\n",
        "\n",
        "def calculate_iibs_score(sentences):\n",
        "    \"\"\"Calculate IIBS - proportion of biased sentences\"\"\"\n",
        "    biased_sentences = 0\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    bias_indicators = ['assume', 'stereotype', 'expect', 'dismiss', 'ignore', 'exclude',\n",
        "                      'less', 'unable', 'cannot', 'avoid', 'prevent', 'restrict']\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_lower = sentence.lower()\n",
        "        has_bias = any(indicator in sentence_lower for indicator in bias_indicators)\n",
        "        if has_bias:\n",
        "            biased_sentences += 1\n",
        "\n",
        "    return biased_sentences / total_sentences if total_sentences > 0 else 0.0\n",
        "\n",
        "def analyze_corpus_bias(csv_file):\n",
        "    \"\"\"Main function to analyze corpus bias\"\"\"\n",
        "    # Load data\n",
        "    df = load_corpus(csv_file)\n",
        "    print(f\"Loaded corpus: {len(df)} sentences, {len(df['Class'].unique())} classes\")\n",
        "\n",
        "    # Define targets\n",
        "    INTERSECTIONAL_TARGETS = define_intersectional_targets()\n",
        "\n",
        "    # Analyze bias sentences only\n",
        "    bias_sentences = df[df['Class'] != 'Neutral']\n",
        "\n",
        "    results = {}\n",
        "    major_classes = list(INTERSECTIONAL_TARGETS.keys())\n",
        "\n",
        "    for class_name in major_classes:\n",
        "        if class_name in INTERSECTIONAL_TARGETS:\n",
        "            class_sentences = bias_sentences[bias_sentences['Class'] == class_name]['Sentence'].tolist()\n",
        "\n",
        "            if len(class_sentences) > 0:\n",
        "                targets = INTERSECTIONAL_TARGETS[class_name]['targets']\n",
        "                pos_attrs = INTERSECTIONAL_TARGETS[class_name]['attributes_positive']\n",
        "                neg_attrs = INTERSECTIONAL_TARGETS[class_name]['attributes_negative']\n",
        "\n",
        "                # Calculate all metrics\n",
        "                ceat_score = calculate_ceat_score(class_sentences, targets, pos_attrs, neg_attrs)\n",
        "                iweat_score = calculate_iweat_score(class_sentences, targets, pos_attrs, neg_attrs)\n",
        "                iseat_score = calculate_iseat_score(class_sentences, targets, pos_attrs, neg_attrs)\n",
        "\n",
        "                results[class_name] = {\n",
        "                    'CEAT': ceat_score,\n",
        "                    'I-WEAT': iweat_score,\n",
        "                    'I-SEAT': iseat_score,\n",
        "                    'sentence_count': len(class_sentences)\n",
        "                }\n",
        "\n",
        "    # Calculate IIBS\n",
        "    all_sentences = df['Sentence'].tolist()\n",
        "    iibs_score = calculate_iibs_score(all_sentences)\n",
        "\n",
        "    # Calculate aggregate scores\n",
        "    aggregate_scores = {'CEAT': [], 'I-WEAT': [], 'I-SEAT': []}\n",
        "    for scores in results.values():\n",
        "        aggregate_scores['CEAT'].append(abs(scores['CEAT']))\n",
        "        aggregate_scores['I-WEAT'].append(abs(scores['I-WEAT']))\n",
        "        aggregate_scores['I-SEAT'].append(abs(scores['I-SEAT']))\n",
        "\n",
        "    final_scores = {\n",
        "        'CEAT': np.mean(aggregate_scores['CEAT']) if aggregate_scores['CEAT'] else 0,\n",
        "        'I-WEAT': np.mean(aggregate_scores['I-WEAT']) if aggregate_scores['I-WEAT'] else 0,\n",
        "        'I-SEAT': np.mean(aggregate_scores['I-SEAT']) if aggregate_scores['I-SEAT'] else 0,\n",
        "        'IIBS': iibs_score\n",
        "    }\n",
        "\n",
        "    return results, final_scores\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Analyze the corpus\n",
        "    results, final_scores = analyze_corpus_bias('corpus.csv')\n",
        "\n",
        "    print(\"=== FINAL BIAS METRICS ===\")\n",
        "    print(f\"CEAT: {final_scores['CEAT']:.2f}\")\n",
        "    print(f\"I-WEAT: {final_scores['I-WEAT']:.2f}\")\n",
        "    print(f\"I-SEAT: {final_scores['I-SEAT']:.2f}\")\n",
        "    print(f\"IIBS: {final_scores['IIBS']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# -------------------\n",
        "# Load RoBERTa encoder\n",
        "# -------------------\n",
        "model = SentenceTransformer(\"roberta-base\")\n",
        "\n",
        "# -------------------\n",
        "# Anchor sets for embeddings\n",
        "# -------------------\n",
        "\n",
        "# CEAT concepts (tech + ethics related)\n",
        "ceat_anchors = {\n",
        "    \"tech\": [\"technology\", \"innovation\", \"AI\", \"machine learning\"],\n",
        "    \"ethics\": [\"ethics\", \"fairness\", \"justice\", \"responsibility\"]\n",
        "}\n",
        "\n",
        "# I-WEAT attributes (example: gender-career bias)\n",
        "iweat_anchors = {\n",
        "    \"male\": [\"man\", \"male\", \"he\", \"him\", \"boy\"],\n",
        "    \"female\": [\"woman\", \"female\", \"she\", \"her\", \"girl\"],\n",
        "    \"career\": [\"career\", \"corporation\", \"salary\", \"office\", \"promotion\"],\n",
        "    \"family\": [\"family\", \"home\", \"children\", \"relatives\", \"caring\"]\n",
        "}\n",
        "\n",
        "# I-SEAT sentence-level anchors\n",
        "iseat_anchors = {\n",
        "    \"positive\": [\"hardworking\", \"capable\", \"intelligent\", \"skilled\"],\n",
        "    \"negative\": [\"lazy\", \"incompetent\", \"ignorant\", \"weak\"]\n",
        "}\n",
        "\n",
        "# IIBS intersectional anchors (race, gender, region, etc.)\n",
        "iibs_anchors = [\n",
        "    \"black woman in tech\",\n",
        "    \"asian man in leadership\",\n",
        "    \"disabled student in education\",\n",
        "    \"immigrant worker in healthcare\",\n",
        "    \"elderly person in workplace\"\n",
        "]\n",
        "\n",
        "# -------------------\n",
        "# Helper: average similarity\n",
        "# -------------------\n",
        "def avg_cosine_similarity(embed, anchor_texts):\n",
        "    anchor_embeds = model.encode(anchor_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    sim = util.cos_sim(embed, anchor_embeds).cpu().numpy()\n",
        "    return float(np.mean(sim))\n",
        "\n",
        "# -------------------\n",
        "# Metric functions\n",
        "# -------------------\n",
        "def calculate_ceat_score(sentence):\n",
        "    sent_embed = model.encode(sentence, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    tech_sim = avg_cosine_similarity(sent_embed, ceat_anchors[\"tech\"])\n",
        "    ethics_sim = avg_cosine_similarity(sent_embed, ceat_anchors[\"ethics\"])\n",
        "    return round(tech_sim - ethics_sim, 4)\n",
        "\n",
        "def calculate_iweat_score(sentence):\n",
        "    sent_embed = model.encode(sentence, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    male_sim = avg_cosine_similarity(sent_embed, iweat_anchors[\"male\"])\n",
        "    female_sim = avg_cosine_similarity(sent_embed, iweat_anchors[\"female\"])\n",
        "    career_sim = avg_cosine_similarity(sent_embed, iweat_anchors[\"career\"])\n",
        "    family_sim = avg_cosine_similarity(sent_embed, iweat_anchors[\"family\"])\n",
        "    # Example: bias = (male-career + female-family) - (male-family + female-career)\n",
        "    return round(((male_sim + career_sim) + (female_sim + family_sim)) - ((male_sim + family_sim) + (female_sim + career_sim)), 4)\n",
        "\n",
        "def calculate_iseat_score(sentence):\n",
        "    sent_embed = model.encode(sentence, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    pos_sim = avg_cosine_similarity(sent_embed, iseat_anchors[\"positive\"])\n",
        "    neg_sim = avg_cosine_similarity(sent_embed, iseat_anchors[\"negative\"])\n",
        "    return round(pos_sim - neg_sim, 4)\n",
        "\n",
        "def calculate_iibs_score(sentence):\n",
        "    sent_embed = model.encode(sentence, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    return round(avg_cosine_similarity(sent_embed, iibs_anchors), 4)\n",
        "\n",
        "# -------------------\n",
        "# Full analysis\n",
        "# -------------------\n",
        "def analyze_bias(sentences, output_csv=\"bias_scores_analysis_embeddings.csv\"):\n",
        "    results = []\n",
        "    for sent in sentences:\n",
        "        ceat = calculate_ceat_score(sent)\n",
        "        iweat = calculate_iweat_score(sent)\n",
        "        iseat = calculate_iseat_score(sent)\n",
        "        iibs = calculate_iibs_score(sent)\n",
        "\n",
        "        results.append({\n",
        "            \"Sentence\": sent,\n",
        "            \"CEAT_Score\": ceat,\n",
        "            \"i-WEAT_Score\": iweat,\n",
        "            \"i-SEAT_Score\": iseat,\n",
        "            \"IIBS_Score\": iibs\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return df\n",
        "\n",
        "# -------------------\n",
        "# Example run\n",
        "# -------------------\n",
        "sentences = [\n",
        "    \"The young immigrant woman struggled to get recognition in the tech company.\",\n",
        "    \"The disabled student faced challenges in accessing online education.\",\n",
        "    \"The elderly man was seen as unfit for leadership roles.\",\n",
        "    \"Women are often assumed to be less technical than men.\",\n",
        "    \"The Asian engineer was overlooked in workplace discussions.\"\n",
        "]\n",
        "\n",
        "df = analyze_bias(sentences)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "dfzNLwjWYeMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9c01ca-8ce7-4389-a23f-2828525fac8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded corpus with latin-1 encoding\n",
            "Loaded 261 sentences across 21 classes\n",
            "\n",
            "Calculating bias scores for 261 sentences...\n",
            "\n",
            "✓ Results saved to: bias_scores_analysis_.csv\n"
          ]
        }
      ]
    }
  ]
}